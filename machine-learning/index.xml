<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-learnings on Simba</title>
    <link>https://HHUsimba.github.io/machine-learning/</link>
    <description>Recent content in Machine-learnings on Simba</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Mon, 01 Jun 2020 18:53:17 +0800</lastBuildDate>
    
	<atom:link href="https://HHUsimba.github.io/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K Means</title>
      <link>https://HHUsimba.github.io/machine-learning/k-means/</link>
      <pubDate>Mon, 01 Jun 2020 18:53:17 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/k-means/</guid>
      <description></description>
    </item>
    
    <item>
      <title>随机森林计算特征重要性</title>
      <link>https://HHUsimba.github.io/machine-learning/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/</link>
      <pubDate>Sat, 30 May 2020 00:04:23 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/</guid>
      <description>一、方法（1）Mean decrease impurity 对于每颗树，按照impurity（gini /entropy /information gain）给特征排序，然后整个森林取平均。最优条件的选择依据是不纯度。不纯度在分类中通常为Gini不纯度或信息增益/信息熵，对于回归问题来说是方差。
基于不纯度对模型进行排序有几点需要注意： （1）基于不纯度降低的特征选择将会偏向于选择那些具有较多类别的变量（bias）。 （2）当存在相关特征时，一个特征被选择后，与其相关的其他特征的重要度则会变得很低，因为他们可以减少的不纯度已经被前面的特征移除了。
sklearn实现如下：
from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor import numpy as np #Load boston housing dataset as an example boston = load_boston() X = boston[&amp;#34;data&amp;#34;] print type(X),X.shape Y = boston[&amp;#34;target&amp;#34;] names = boston[&amp;#34;feature_names&amp;#34;] print names rf = RandomForestRegressor() rf.fit(X, Y) print &amp;#34;Features sorted by their score:&amp;#34; print sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True) 结果：
Features sorted by their score: [(0.</description>
    </item>
    
    <item>
      <title>数据预处理问题</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sat, 18 May 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/</guid>
      <description>归一化/标准化的目标 数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。
1 把数变为（0，1）之间的小数 主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。 2 把有量纲表达式变为无量纲表达式 归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
法一：min-max归一化(Min-max normalization) 　也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下： 其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。 用处：脏值数据处理好之后，进行min-max归一化处理，然后输入到网络中预测降雨。
法二：z-score 标准化(zero-mean normalization) 　也叫标准差标准化，经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为： 其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
———————————————————————————————————————————— （2020.6.12更新）
问题情景：对于数据的大值预测不准，优化方案：首先加归一化方法，然后模型调优，由于激活函数用的是sigmoid，故用归一化
用法： 如果对输出结果范围有要求，用归一化 如果数据较为稳定，不存在极端的最大最小值，用归一化 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响 一般来说，建议优先使用标准化。在对输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出调整到 0-1，如果我们对于数据的分布有假设的话，更加有效方法是使用相对应的概率密度函数来转换</description>
    </item>
    
    <item>
      <title>Idea</title>
      <link>https://HHUsimba.github.io/machine-learning/idea/</link>
      <pubDate>Tue, 07 May 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/idea/</guid>
      <description>找一些图像定位的其他领域的文章，学习一下，即追踪图形变换
5.7 5.9 </description>
    </item>
    
    <item>
      <title>激活函数总结</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</link>
      <pubDate>Thu, 02 May 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</guid>
      <description>CNN/RNN各种模型激活函数总结
0.Sigmoid： Sigmoid函数是早期非常经典的激活函数，输出值在 [0,1] 之间，当输入值为非常大负数时，值为0，当输入值非常大正数时，值为1。Sigmoid非常经典，但是现在它以不太受欢迎，原因是它存在一个几个比较大的缺陷，后面做详细讨论。 1.Tanh Tanh函数是Sigmoid函数的一种变种，取值范围为 [-1 , 1]，它解决了Sigmoid函数的非0均值的问题，公式、图像以及导数图像如下图所示： 2.Relu 目前比较受欢迎的激活函数是Relu，公式简单，但是非常好用，公式如下所示，输入小于0时输出为0，大于0时输出为本身。 Relu解决的问题： 1、解决了梯度消失的问题 2、公式简单没有指数运算，计算快 3、x&amp;gt;0时，梯度为1，无梯度耗散问题，收敛速度快 Relu存在的问题： 可能出现神经元死亡，权重无法更新：首先复习前向传播和反向传播。</description>
    </item>
    
    <item>
      <title>服务推荐的多维QoS预测</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%9C%8D%E5%8A%A1%E6%8E%A8%E8%8D%90%E7%9A%84%E5%A4%9A%E7%BB%B4qos%E9%A2%84%E6%B5%8B/</link>
      <pubDate>Fri, 22 Mar 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%9C%8D%E5%8A%A1%E6%8E%A8%E8%8D%90%E7%9A%84%E5%A4%9A%E7%BB%B4qos%E9%A2%84%E6%B5%8B/</guid>
      <description>摘要： 移动互联网技术的进步使得Web服务的客户能够在移动时保持其服务会话的活跃性。由于移动客户端消费的服务可能由于客户端位置变化而随时间不同，因此分析服务消费关系需要多维时空模型。此外，用于移动客户端的竞争性Web服务推荐器必须能够通过考虑目标客户端的服务请求时间和位置来很好地预测未知的服务质量（QoS）值，例如，通过一组多个来执行预测。维度QoS措施。大多数现代QoS预测方法利用一个特定维度的QoS特征，例如时间或位置，并且不利用多维QoS数据之间的结构关系。本文提出了一种集成的QoS预测方法，该方法通过基于多线性代数的张量概念统一了多维QoS数据的建模，并通过张量分解和重构优化算法为移动客户端提供高效的Web服务推荐。鉴于公共领域中测量的多维QoS数据集不可用，本文还提出了一种转换方法，用于从包含高维时间和空间信息的测量出租车使用数据集创建可靠的多维QoS数据集。
介绍： Web服务定义：是API定义的软件组件
功能：可以通过网络按需组合和交付给客户端。当有许多候选服务要评估能力传递请求时，服务建议对于有效选择最佳服务至关重要。大多数服务推荐方法都考虑了服务质量（QoS）特性。
客户在移动时保持其服务会话的活跃性。由于客户端位置变化导致移动客户端消费的服务可能随时间而不同，因此分析服务消费关系需要多维时空模型。为了获得最佳服务建议，在预测必要的未知QoS值时，必须尽可能地利用历史多维QoS数据。
数据集的结构：用户（或客户端），服务和时间段的三维QoS： 有m个用户和n个 Web服务。u i和s j分别表示第i个用户和第j个服务。Ti表示第k个时间段。如果在特定时间段内调用服务，则在该时间段内记录调用的QoS值（例如，响应时间）。
仅示出了一个QoS属性。实际上，可以在一个QoS数据集中捕获多个QoS属性，并将每个属性建模为单独的维度。此外，当需要考虑服务请求位置时，可以通过另一维度对位置数据进行建模。因此，所有这些QoS数据可以形成五维空间，下图所示（其中每个长方体是图1的三维QoS数据）。
现有研究：尽管许多先前的研究已经认识到需要利用多维QoS数据，但是数据没有以集成的方式使用。最常见的方法是分别预测每个QoS属性，而不考虑时间，位置和其他因素更具体地说，大多数相关的工作集中在时间特征或位置，但没有考虑从用户（或服务）的角度来看所有QoS数据之间的依赖关系客户）。
现有研究的缺点: 1）多维QoS数据的整体结构被忽略用于QoS预测，这会损害更准确的预测结果;
2）使用特定于一维的特征的QoS预测方法难以扩展到其他维度，这需要合适的预测方法来考虑所有QoS维度的特征是复杂的;
3）当需要额外的维度时（例如，由于新的网络计算技术的部署），将需要设计新的预测方法。
本文提出的新方法： 一种集成的QoS预测方法（称为HDOP），该方法利用面向Web服务推荐的面向高维的QoS预测方法。
优点：
（1）通过基于多线性代数的张量概念统一了多维QoS数据的建模，该概念整体地考虑了多维QoS数据的挛缩;
（2）采用有效的优化算法进行张量分解和重构;
（3）实现准确的QoS预测。
方法设计原理：  HDOP旨在通过统一地并以集成方式考虑所有QoS维度，从而能够容易且有效地准确预测任何维度中的未知QoS值。为了实现目标，我们需要决定（1）如何建模多维QoS数据和（2）如何使用QoS数据模型进行预测。  基础知识：  张量性质 张良分解  使用张量模型的QoS预测
1.建立QOS张量
2.计算组件矩阵
3.制定预测
4.计算复杂性分析
用于QOS预测的新的数据集：
 数据集基本原理 数据集的转换 数据集的开发  数据集的局限性： 尽管HDOP非常有效和准确，但它在实践中具有以下局限性：
 HDOP将整个QoS数据建模为张量，因此并行实现是不容易的。传统的矩阵分解方法可以将多维QoS建模为多个矩阵，并同时处理每个矩阵。 当QoS数据非常稀疏时，QoS张量可能需要比多个QoS矩阵更多的存储或存储空间。  结论和未来的工作: 我们利用张量来预测高维时空空间中的未知QoS值，以集成的方式考虑所有QoS维度，以准确且容易地预测多维QoS。我们提出的方法首先将多维QoS数据建模为张量，然后通过分解该QoS张量来找出其组件矩阵。这些组件矩阵允许我们准确地重建QoS张量。最后，重构张量告诉我们对QoS数据的未知值的预测。与相关工作不同，我们提出的方法充分利用了多维QoS数据的整体结构，可以方便，准确地预测任何维度的QoS数据。</description>
    </item>
    
    <item>
      <title>Matplotlib整理</title>
      <link>https://HHUsimba.github.io/machine-learning/matplotlib%E6%95%B4%E7%90%86/</link>
      <pubDate>Sat, 02 Mar 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/matplotlib%E6%95%B4%E7%90%86/</guid>
      <description>画图时整理的matplotlib的常用知识点，方便查找
0.导入： import matplotlib as mpl import matplotlib.pyplot as plt 1.plot() 函数功能：展现变量的趋势变化。 调用：
plt.plot(s, y, ls=&amp;#34;-&amp;#34;, lw=2, label=&amp;#34;plot figure&amp;#34;) 参数说明： x：x轴上的数值。 y：y轴上的数值。 ls：折线图的线条风格。 lw：折线图的线条宽度。 label：标记图形内容的标签文本。
2.scatter() 函数功能：寻找变量之间的关系。 调用：
plt.scatter(x, y, c=&amp;#34;b&amp;#34;, label=&amp;#34;scatter figure&amp;#34;) 参数说明： x：x轴上的数值。 y：y轴上的数值。 c：散点图中标记的颜色。 label：标记图形内容的标签文本。
3.xlim() &amp;amp; ylim() 函数功能：设置x轴（y轴）的数值显示范围。 调用：
plt.xlim(xmin, xmax) 参数说明： xmin：x轴上的最小值。 xmax：x轴上的最大值。 平移性：上面的函数功能，调用签名和参数说明同样可以平移到函数ylim()上。
4.xlabel() &amp;amp; ylabel() 函数功能：设置x轴（y轴）的标签文本。 调用：
plt.xlabel(string) 参数说明： string：标签文本内容。 平移性：上面的函数功能，调用签名和参数说明同样可以平移到函数ylabel()上。
5.grid() 函数功能：绘制刻度线的网格线。 调用：
ply.grid(linestyle=&amp;#34;:&amp;#34;, color=&amp;#34;r&amp;#34;) 参数说明： linestyle：网格线的线条风格。 color：网格线的线条颜色。
6.axhline() &amp;amp; axvline() 函数功能：绘制平行于x轴（y轴）的水平（竖直）参考线。 调用签名：</description>
    </item>
    
    <item>
      <title>应用场景 气象预报</title>
      <link>https://HHUsimba.github.io/machine-learning/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-%E6%B0%94%E8%B1%A1%E9%A2%84%E6%8A%A5/</link>
      <pubDate>Wed, 21 Nov 2018 10:47:06 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-%E6%B0%94%E8%B1%A1%E9%A2%84%E6%8A%A5/</guid>
      <description>应用场景：气象预报 1. 预报类别（按照时间）    预报类别 时间     临近预报 0-2小时内   短临预报 2-12小时内   短期预报 1-3天   中期预报 3-15天   长期预报 15天以上    参考：中国气象局官网、临近预报
http://www.cma.gov.cn/2011xwzx/2011xqxxw/2011xqxyw/201408/t20140822_258683.html
https://baike.baidu.com/item/%E4%B8%B4%E8%BF%91%E9%A2%84%E6%8A%A5/5021658
2. 预报类别（按照方法） </description>
    </item>
    
    <item>
      <title>降雨预测方法</title>
      <link>https://HHUsimba.github.io/machine-learning/%E9%99%8D%E9%9B%A8%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95/</link>
      <pubDate>Wed, 21 Nov 2018 10:47:06 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E9%99%8D%E9%9B%A8%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95/</guid>
      <description>降雨预测方法   DBNPF (Deep Belief Network for Precipitation Forecast) 来源：张雷师兄论文：A deep-learning based precipitation forecasting
  模型： 比较：RBF、SVM、ARIMA、ELM（extreme learning machine）、SAE（Sparse AutoEncoder） 数据集： 遵义市1956-2010
train data:1956-2000
test data:2000-2010
  动态区域组合MLP 来源：贾旸旸师兄论文：Short-term Rainfall Forecasting Using Multi-layer Perceptron
  模型： PCA：13个物理因子进行降维，输入到MLP中
贪婪算法决定MLP的结构，
该模型的初始数据包括五个高空因素和八个地表因素。
在气象学中，通常用位势高度代替实际高度，用等压面代替水平高度，因此，气象数据总是采用等压面格式。例如，500hpa通常相当于5.5km的高度。降雨系统通常由500hpa的天气系统控制。根据区域经验，该模型选择的五个海拔因子分别是500hpa高度下的实际高度（x1）、温度（x2）、温度露点差（x3）、风向（x4）和风速（x5）。风向和风速影响着降雨系统的运动方向和速度。温度露点差与湿度直接相关。温度露点差、温度和实际高度值影响着降雨系统的内能。地表因子代表该地区的局部大气条件。不同地区地表因子的差异导致降雨不同。该模型中所用的八个面因子包括总云量（X6）、地表风速（X7）、地面风向（X8）、地面气压（X9）、地表3小时压力变化（X10）、地表温度露点差（X11）、地表温度（X12）和过去三小时的降雨。周围区域（x13）。对于同一个预测区域，每个周边区域都与该预测区域建立一个MLP。表1显示了所有13个因素。这些因素是我们模型的初始输入。
最小-最大规范化。最常用的数据规范化方法之一是最小-最大规范化。它可以在0和1之间标准化数据。由于不同因素的大小不同，有必要对数据进行预处理。对于要处理的序列，序列的最大值对应于1，最小值对应于0，其余值在0和1之间按比例转换。
主成分分析。归一化后，PCA用于减小输入的维数。确定新因子个数的标准是99%，即所选因子的特征值之和占总特征值的99%以上。经计算，新因子的总信息可以代表原始数据的99%以上。此标准定义了保留的信息量，但没有指定所需的因子数量。对于不同的预测区域，因子的数量可能不同，但不会超过初始输入，即13。在大多数情况下，需要的因素数量在3到8之间。PCA处理后，所需的计算资源大大减少。
MLP的输入是Z1-Z4四个参数，输出是降雨量
step1:
step2:中心预测点与其他地区各有一个MLP，两个地区的距离决定了周围MLP的数量。周围多个MLP模型一起决策，如果预测降雨的MLP超过1，取均值。
比较： 数据集： 2015-2017年海拔（500hPa）测绘数据和数值预报结果。
train data:2015-2016
test data:2016-2017
  基于雷达回波图像的短期降雨预测 来源：基于雷达回波图像的短期降雨预测
  模型： 卷积自编码器的编码模块首先提取每帧输入图像特征，送入LSTM预报网络；LSTM预报网络的编码模块，对输入信息提取时序特征，在此基础上，由LSTM预测模块产生关于未来时段回波图像时序特征预测。
比较： 在MINIST数据集上对自编码器的层数和LSTM层数预测效果进行对比
数据集： 石家庄地区 2010 -2017 年之间降雨天气的雷达回波图像 。</description>
    </item>
    
    <item>
      <title>Latex技巧总结</title>
      <link>https://HHUsimba.github.io/machine-learning/latex%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 03 Sep 2018 08:46:13 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/latex%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/</guid>
      <description>导包: 在LaTex中有专门的导包的地方：   修改表格线条粗细\hline
step1: 用 \usepackage{booktabs} 导包 step2: 首行: \toprule[1pt] 中间行：\midrule[1pt] 尾行：\midrule[1pt]
  表格内容对齐：
\begin{tabular}{c} c:center r:right l:left
  Latex添加新一页 用“\clearpage” 不要用“\newpage”
  调整表格列间距
\begin{tabular}{|p{1cm}|p{2cm}|p{3cm}|}   图表位置移动
\begin{table}[!htb] 首先，!表示无视美学规则，按下面的要求放置； 然后h t b p分别代表4中放置方式，优先级依次递减。 可以自己定优先级，tbhp等 常用选项[htbp]是浮动格式： h=here 放置在代码中这个图片出现的位置，也就是你想让它出现的位置； t = top 放在该页顶端； b = bottom 放在该页底部； p = page浮动页。将图形放置在一只允许有浮动对象的页面上。 一般使用[htb]这样的组合，只用[h]是没有用的。这样组合的意思就是latex会尽量满足排在前面的浮动格	式，就是h-t-b这个顺序，让排版的效果尽量好。
!h 只是试图放在当前位置。如果页面剩下的部分放不下，还是会跑到下一页的。一般页言，用 [!h] 选项经常会出现不能正确放置的问题，所以常用 [ht]、[htbp] 等。
  增大表格行距
\begin{document} \renewcommand\arraystretch{2} //作用是讲每一行的高度变为原来的两倍。 \begin{table}[h] \centering   表格内容调整</description>
    </item>
    
  </channel>
</rss>