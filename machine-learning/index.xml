<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-learnings on Simba</title>
    <link>https://HHUsimba.github.io/machine-learning/</link>
    <description>Recent content in Machine-learnings on Simba</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Mon, 01 Jun 2020 18:53:17 +0800</lastBuildDate>
    
	<atom:link href="https://HHUsimba.github.io/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K Means</title>
      <link>https://HHUsimba.github.io/machine-learning/k-means/</link>
      <pubDate>Mon, 01 Jun 2020 18:53:17 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/k-means/</guid>
      <description></description>
    </item>
    
    <item>
      <title>随机森林计算特征重要性</title>
      <link>https://HHUsimba.github.io/machine-learning/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/</link>
      <pubDate>Sat, 30 May 2020 00:04:23 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%AE%A1%E7%AE%97%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/</guid>
      <description>一、方法（1）Mean decrease impurity 对于每颗树，按照impurity（gini /entropy /information gain）给特征排序，然后整个森林取平均。最优条件的选择依据是不纯度。不纯度在分类中通常为Gini不纯度或信息增益/信息熵，对于回归问题来说是方差。
基于不纯度对模型进行排序有几点需要注意： （1）基于不纯度降低的特征选择将会偏向于选择那些具有较多类别的变量（bias）。 （2）当存在相关特征时，一个特征被选择后，与其相关的其他特征的重要度则会变得很低，因为他们可以减少的不纯度已经被前面的特征移除了。
sklearn实现如下：
from sklearn.datasets import load_boston from sklearn.ensemble import RandomForestRegressor import numpy as np #Load boston housing dataset as an example boston = load_boston() X = boston[&amp;#34;data&amp;#34;] print type(X),X.shape Y = boston[&amp;#34;target&amp;#34;] names = boston[&amp;#34;feature_names&amp;#34;] print names rf = RandomForestRegressor() rf.fit(X, Y) print &amp;#34;Features sorted by their score:&amp;#34; print sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True) 结果：
Features sorted by their score: [(0.</description>
    </item>
    
    <item>
      <title>随机森林调参</title>
      <link>https://HHUsimba.github.io/machine-learning/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%B0%83%E5%8F%82/</link>
      <pubDate>Mon, 06 Apr 2020 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E8%B0%83%E5%8F%82/</guid>
      <description>Sklearn.ensemble.RandomForstClassifier 参数说明 Sklearn.ensemble.RandomForstClassifier(n_estimators=10, criterion=’gini’, max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1,random_state=None, verbose=0, warm_start=False, class_weight=None) *n_estimators* : integer, optional (default=10)
随机森林树的数目
*criterion* : string, optional (default=”gini”)
衡量分裂质量的功能。 支持的标准是基尼杂质的“gini”和信息增益的“熵”。 注意：此参数是特定于树的。
*max_features* : int, float, string or None, optional (default=”auto”)
寻找最佳分割时要考虑的功能数量：
如果是int，则在每次拆分时考虑max_features功能。
如果为float，则max_features为百分比，并在每次拆分时考虑int（max_features * n_features）要素。
如果是“auto”，则max_features = sqrt（n_features）。
如果是“sqrt”，则max_features = sqrt（n_features）（与“auto”相同）。
如果是“log2”，则max_features = log2（n_features）。
如果为None，则max_features = n_features。
注意：在找到节点样本的至少一个有效分区之前，搜索分割不会停止，即使它需要有效地检查超过max_features的功能。
*max_depth* : integer or None, optional (default=None)
树的最大深度。 如果为None，则扩展节点直到所有叶子都是纯的或直到所有叶子包含少于min_samples_split样本。
*min_samples_split* : int, float, optional (default=2)</description>
    </item>
    
    <item>
      <title>梯度优化方法</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</link>
      <pubDate>Sat, 02 Nov 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</guid>
      <description>0.SGD(最常用)： 此处的SGD指mini-batch gradient descent，关于batch gradient descent, stochastic gradient descent, 以及 mini-batch gradient descent的具体区别就不细说了。现在的SGD一般都指mini-batch gradient descent。
SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：
其中，是学习率，是梯度 SGD完全依赖于当前batch的梯度，所以可理解为允许当前batch的梯度多大程度影响参数更新
缺点：
  选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了
  SGD容易收敛到局部最优
  1.Momentum momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下：
其中，是动量因子
特点：
 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的能够进行很好的加速 下降中后期时，在局部最小值来回震荡的时候，，使得更新幅度增大，跳出陷阱 在梯度改变方向的时候，能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛  2.Nesterov nesterov项在梯度更新时做一个校正，避免前进太快，同时提高灵敏度。 将上一节中的公式展开可得：
可以看出，并没有直接改变当前梯度，所以Nesterov的改进就是让之前的动量直接影响当前的动量。即：
3.Adagrad Adagrad其实是对学习率进行了一个约束。即：
此处，对从1到进行一个递推形成一个约束项regularizer，，用来保证分母非0
特点：
 前期较小的时候， regularizer较大，能够放大梯度 后期较大的时候，regularizer较小，能够约束梯度 适合处理稀疏梯度  缺点：
 由公式可以看出，仍依赖于人工设置一个全局学习率 设置过大的话，会使regularizer过于敏感，对梯度的调节太大 中后期，分母上梯度平方的累加将会越来越大，使，使得训练提前结束  4.Adadelta Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。 Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。即：
在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后：
其中，代表求期望。
此时，可以看出Adadelta已经不用依赖于全局学习率了。
特点：
 训练初中期，加速效果不错，很快 训练后期，反复在局部最小值附近抖动  5.</description>
    </item>
    
    <item>
      <title>数据预处理问题</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sat, 18 May 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/</guid>
      <description>归一化/标准化的目标 数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。
1 把数变为（0，1）之间的小数 主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速，应该归到数字信号处理范畴之内。 2 把有量纲表达式变为无量纲表达式 归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
法一：min-max归一化(Min-max normalization) 　也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下： 其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。 用处：脏值数据处理好之后，进行min-max归一化处理，然后输入到网络中预测降雨。
法二：z-score 标准化(zero-mean normalization) 　也叫标准差标准化，经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为： 其中μ为所有样本数据的均值，σ为所有样本数据的标准差。
———————————————————————————————————————————— （2020.6.12更新）
问题情景：对于数据的大值预测不准，优化方案：首先加归一化方法，然后模型调优，由于激活函数用的是sigmoid，故用归一化
用法： 如果对输出结果范围有要求，用归一化 如果数据较为稳定，不存在极端的最大最小值，用归一化 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响 一般来说，建议优先使用标准化。在对输出有要求时再尝试别的方法，如归一化或者更加复杂的方法。很多方法都可以将输出调整到 0-1，如果我们对于数据的分布有假设的话，更加有效方法是使用相对应的概率密度函数来转换</description>
    </item>
    
    <item>
      <title>Idea</title>
      <link>https://HHUsimba.github.io/machine-learning/idea/</link>
      <pubDate>Tue, 07 May 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/idea/</guid>
      <description>找一些图像定位的其他领域的文章，学习一下，即追踪图形变换
5.7 5.9 </description>
    </item>
    
    <item>
      <title>激活函数总结</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</link>
      <pubDate>Thu, 02 May 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</guid>
      <description>CNN/RNN各种模型激活函数总结
0.Sigmoid： Sigmoid函数是早期非常经典的激活函数，输出值在 [0,1] 之间，当输入值为非常大负数时，值为0，当输入值非常大正数时，值为1。Sigmoid非常经典，但是现在它以不太受欢迎，原因是它存在一个几个比较大的缺陷，后面做详细讨论。 1.Tanh Tanh函数是Sigmoid函数的一种变种，取值范围为 [-1 , 1]，它解决了Sigmoid函数的非0均值的问题，公式、图像以及导数图像如下图所示： 2.Relu 目前比较受欢迎的激活函数是Relu，公式简单，但是非常好用，公式如下所示，输入小于0时输出为0，大于0时输出为本身。 Relu解决的问题： 1、解决了梯度消失的问题 2、公式简单没有指数运算，计算快 3、x&amp;gt;0时，梯度为1，无梯度耗散问题，收敛速度快 Relu存在的问题： 可能出现神经元死亡，权重无法更新：首先复习前向传播和反向传播。</description>
    </item>
    
    <item>
      <title>服务推荐的多维QoS预测</title>
      <link>https://HHUsimba.github.io/machine-learning/%E6%9C%8D%E5%8A%A1%E6%8E%A8%E8%8D%90%E7%9A%84%E5%A4%9A%E7%BB%B4qos%E9%A2%84%E6%B5%8B/</link>
      <pubDate>Fri, 22 Mar 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E6%9C%8D%E5%8A%A1%E6%8E%A8%E8%8D%90%E7%9A%84%E5%A4%9A%E7%BB%B4qos%E9%A2%84%E6%B5%8B/</guid>
      <description>摘要： 移动互联网技术的进步使得Web服务的客户能够在移动时保持其服务会话的活跃性。由于移动客户端消费的服务可能由于客户端位置变化而随时间不同，因此分析服务消费关系需要多维时空模型。此外，用于移动客户端的竞争性Web服务推荐器必须能够通过考虑目标客户端的服务请求时间和位置来很好地预测未知的服务质量（QoS）值，例如，通过一组多个来执行预测。维度QoS措施。大多数现代QoS预测方法利用一个特定维度的QoS特征，例如时间或位置，并且不利用多维QoS数据之间的结构关系。本文提出了一种集成的QoS预测方法，该方法通过基于多线性代数的张量概念统一了多维QoS数据的建模，并通过张量分解和重构优化算法为移动客户端提供高效的Web服务推荐。鉴于公共领域中测量的多维QoS数据集不可用，本文还提出了一种转换方法，用于从包含高维时间和空间信息的测量出租车使用数据集创建可靠的多维QoS数据集。
介绍： Web服务定义：是API定义的软件组件
功能：可以通过网络按需组合和交付给客户端。当有许多候选服务要评估能力传递请求时，服务建议对于有效选择最佳服务至关重要。大多数服务推荐方法都考虑了服务质量（QoS）特性。
客户在移动时保持其服务会话的活跃性。由于客户端位置变化导致移动客户端消费的服务可能随时间而不同，因此分析服务消费关系需要多维时空模型。为了获得最佳服务建议，在预测必要的未知QoS值时，必须尽可能地利用历史多维QoS数据。
数据集的结构：用户（或客户端），服务和时间段的三维QoS： 有m个用户和n个 Web服务。u i和s j分别表示第i个用户和第j个服务。Ti表示第k个时间段。如果在特定时间段内调用服务，则在该时间段内记录调用的QoS值（例如，响应时间）。
仅示出了一个QoS属性。实际上，可以在一个QoS数据集中捕获多个QoS属性，并将每个属性建模为单独的维度。此外，当需要考虑服务请求位置时，可以通过另一维度对位置数据进行建模。因此，所有这些QoS数据可以形成五维空间，下图所示（其中每个长方体是图1的三维QoS数据）。
现有研究：尽管许多先前的研究已经认识到需要利用多维QoS数据，但是数据没有以集成的方式使用。最常见的方法是分别预测每个QoS属性，而不考虑时间，位置和其他因素更具体地说，大多数相关的工作集中在时间特征或位置，但没有考虑从用户（或服务）的角度来看所有QoS数据之间的依赖关系客户）。
现有研究的缺点: 1）多维QoS数据的整体结构被忽略用于QoS预测，这会损害更准确的预测结果;
2）使用特定于一维的特征的QoS预测方法难以扩展到其他维度，这需要合适的预测方法来考虑所有QoS维度的特征是复杂的;
3）当需要额外的维度时（例如，由于新的网络计算技术的部署），将需要设计新的预测方法。
本文提出的新方法： 一种集成的QoS预测方法（称为HDOP），该方法利用面向Web服务推荐的面向高维的QoS预测方法。
优点：
（1）通过基于多线性代数的张量概念统一了多维QoS数据的建模，该概念整体地考虑了多维QoS数据的挛缩;
（2）采用有效的优化算法进行张量分解和重构;
（3）实现准确的QoS预测。
方法设计原理：  HDOP旨在通过统一地并以集成方式考虑所有QoS维度，从而能够容易且有效地准确预测任何维度中的未知QoS值。为了实现目标，我们需要决定（1）如何建模多维QoS数据和（2）如何使用QoS数据模型进行预测。  基础知识：  张量性质 张良分解  使用张量模型的QoS预测
1.建立QOS张量
2.计算组件矩阵
3.制定预测
4.计算复杂性分析
用于QOS预测的新的数据集：
 数据集基本原理 数据集的转换 数据集的开发  数据集的局限性： 尽管HDOP非常有效和准确，但它在实践中具有以下局限性：
 HDOP将整个QoS数据建模为张量，因此并行实现是不容易的。传统的矩阵分解方法可以将多维QoS建模为多个矩阵，并同时处理每个矩阵。 当QoS数据非常稀疏时，QoS张量可能需要比多个QoS矩阵更多的存储或存储空间。  结论和未来的工作: 我们利用张量来预测高维时空空间中的未知QoS值，以集成的方式考虑所有QoS维度，以准确且容易地预测多维QoS。我们提出的方法首先将多维QoS数据建模为张量，然后通过分解该QoS张量来找出其组件矩阵。这些组件矩阵允许我们准确地重建QoS张量。最后，重构张量告诉我们对QoS数据的未知值的预测。与相关工作不同，我们提出的方法充分利用了多维QoS数据的整体结构，可以方便，准确地预测任何维度的QoS数据。</description>
    </item>
    
    <item>
      <title>Matplotlib整理</title>
      <link>https://HHUsimba.github.io/machine-learning/matplotlib%E6%95%B4%E7%90%86/</link>
      <pubDate>Sat, 02 Mar 2019 22:59:22 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/matplotlib%E6%95%B4%E7%90%86/</guid>
      <description>画图时整理的matplotlib的常用知识点，方便查找
0.导入： import matplotlib as mpl import matplotlib.pyplot as plt 1.plot() 函数功能：展现变量的趋势变化。 调用：
plt.plot(s, y, ls=&amp;#34;-&amp;#34;, lw=2, label=&amp;#34;plot figure&amp;#34;) 参数说明： x：x轴上的数值。 y：y轴上的数值。 ls：折线图的线条风格。 lw：折线图的线条宽度。 label：标记图形内容的标签文本。
2.scatter() 函数功能：寻找变量之间的关系。 调用：
plt.scatter(x, y, c=&amp;#34;b&amp;#34;, label=&amp;#34;scatter figure&amp;#34;) 参数说明： x：x轴上的数值。 y：y轴上的数值。 c：散点图中标记的颜色。 label：标记图形内容的标签文本。
3.xlim() &amp;amp; ylim() 函数功能：设置x轴（y轴）的数值显示范围。 调用：
plt.xlim(xmin, xmax) 参数说明： xmin：x轴上的最小值。 xmax：x轴上的最大值。 平移性：上面的函数功能，调用签名和参数说明同样可以平移到函数ylim()上。
4.xlabel() &amp;amp; ylabel() 函数功能：设置x轴（y轴）的标签文本。 调用：
plt.xlabel(string) 参数说明： string：标签文本内容。 平移性：上面的函数功能，调用签名和参数说明同样可以平移到函数ylabel()上。
5.grid() 函数功能：绘制刻度线的网格线。 调用：
ply.grid(linestyle=&amp;#34;:&amp;#34;, color=&amp;#34;r&amp;#34;) 参数说明： linestyle：网格线的线条风格。 color：网格线的线条颜色。
6.axhline() &amp;amp; axvline() 函数功能：绘制平行于x轴（y轴）的水平（竖直）参考线。 调用签名：</description>
    </item>
    
    <item>
      <title>应用场景 气象预报</title>
      <link>https://HHUsimba.github.io/machine-learning/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-%E6%B0%94%E8%B1%A1%E9%A2%84%E6%8A%A5/</link>
      <pubDate>Wed, 21 Nov 2018 10:47:06 +0800</pubDate>
      
      <guid>https://HHUsimba.github.io/machine-learning/%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-%E6%B0%94%E8%B1%A1%E9%A2%84%E6%8A%A5/</guid>
      <description>应用场景：气象预报 1. 预报类别（按照时间）    预报类别 时间     临近预报 0-2小时内   短临预报 2-12小时内   短期预报 1-3天   中期预报 3-15天   长期预报 15天以上    参考：中国气象局官网、临近预报
http://www.cma.gov.cn/2011xwzx/2011xqxxw/2011xqxyw/201408/t20140822_258683.html
https://baike.baidu.com/item/%E4%B8%B4%E8%BF%91%E9%A2%84%E6%8A%A5/5021658
2. 预报类别（按照方法） </description>
    </item>
    
  </channel>
</rss>